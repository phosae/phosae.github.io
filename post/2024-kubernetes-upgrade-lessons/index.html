<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训 - ZengXu&#39;s BLOG</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="Zeng Xu" /><meta name="description" content="Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训" /><meta name="keywords" content="kubernetes" />






<meta name="generator" content="Hugo 0.125.0 with theme even" />


<link rel="canonical" href="https://www.zeng.dev/post/2024-kubernetes-upgrade-lessons/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">



<link href="/sass/main.min.3ab191e0444a0833d62fa8f1e44231fc793f2c04a2474a8b9348894c550f8388.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:url" content="https://www.zeng.dev/post/2024-kubernetes-upgrade-lessons/">
  <meta property="og:site_name" content="ZengXu&#39;s BLOG">
  <meta property="og:title" content="Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训">
  <meta property="og:description" content="Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
  <meta property="article:section" content="post">
    <meta property="article:published_time" content="2024-03-26T15:31:54+08:00">
    <meta property="article:modified_time" content="2024-04-10T17:07:07+08:00">
    <meta property="article:tag" content="Kubernetes">

  <meta itemprop="name" content="Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训">
  <meta itemprop="description" content="Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训">
  <meta itemprop="datePublished" content="2024-03-26T15:31:54+08:00">
  <meta itemprop="dateModified" content="2024-04-10T17:07:07+08:00">
  <meta itemprop="wordCount" content="3033">
  <meta itemprop="keywords" content="kubernetes"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训">
<meta name="twitter:description" content="Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训">

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Zeng Xu&#39;s BLOG</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">主页</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">归档</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">标签</li>
      </a><a href="/about">
        <li class="mobile-menu-item">我</li>
      </a>
  </ul>

  


</nav>

  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Zeng Xu&#39;s BLOG</a>
</div>





<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">主页</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">归档</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">标签</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about">我</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训</h1>

      <div class="post-meta">
        <span class="post-time"> 2024-03-26 15:31 </span>
        
        
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#container-runtime-切换及升级">Container Runtime 切换及升级</a></li>
    <li><a href="#使用脚本简化操作">使用脚本简化操作</a></li>
    <li><a href="#经验---新-runtime-无法自-registry-拉取镜像">经验 - 新 Runtime 无法自 Registry 拉取镜像</a></li>
    <li><a href="#教训---所有-ingress-节点同时升级导致全集群业务不可用">教训 - 所有 Ingress 节点同时升级导致全集群业务不可用</a></li>
    <li><a href="#教训---boundserviceaccounttokenvolume-变动导致-ingress-nginx-新建容器启动失败">教训 - BoundServiceAccountTokenVolume 变动导致 Ingress Nginx 新建容器启动失败</a></li>
    <li><a href="#裸金属环境-apiserver-真是高可用吗">裸金属环境 apiserver 真是高可用吗</a></li>
    <li><a href="#经验---runtime-切换之-gpu-节点处理">经验 - Runtime 切换之 GPU 节点处理</a></li>
    <li><a href="#结语设计可持续演进的软件体系">结语：设计可持续演进的软件体系</a></li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p>距 <a href="../2023-kubernetes-upgrade-1.14-1.21">v1.14.5 - v1.21.14 Kubernetes 跨版本升级记录</a> 发布，忽忽几月。</p>
<p>降本增效大环境下，一边做视频云业务，一边抽空搞 K8s 升级，陆续搞定了 10 来个生产集群的更新。</p>
<p>期间将 Docker 切换成了 containerd，完成了 Container Runtime 更新。同时也拔出萝卜带出泥，遇到不少事故。</p>
<p>下文开始分节补遗并总结经验教训。</p>
<h2 id="container-runtime-切换及升级">Container Runtime 切换及升级</h2>
<p>使用 <a href="../2023-kubernetes-upgrade-1.14-1.21">前文</a> 方法实验完一个小集群之后，经团队讨论，一致认定 Container Runtime 为 K8s 集群的重要基础组件，新版本会包括功能迭代、漏洞修复等，理应持续更新。</p>
<p>旧集群运行使用 dockerd 版本 18.06.2-ce，构建时间 2019/02，竟是 5 年前版本了。</p>
<p>最为燃眉的是，该版本偶发无法清理 Sandbox Container BUG，直观感受是 Pod 一直处于 Terminating 状态。</p>
<p><img src="/img/2024/pod-stuck-in-terminating.jpg" alt=""></p>
<p>原因是 dockerd 不响应个别 kubelet KillPodSandbox 请求，导致无法完成清理。</p>
<pre><code>Warning FailedKillPod 25m kubelet, woker1188 error killing pod: failed to &quot;KillPodSandbox&quot; 
&quot;t231206-1451-07952-t7vr-6b9b89df7b-2vkzp&quot; with KillPodSandboxError: &quot;rpc error: code = DeadlineExceeded 
desc = context deadline exceeded&quot;
Warning FailedSync 2m48s (x36 over 22m) kubelet, woker1188 error determining status: rpc error: code = 
DeadlineExceeded desc = context deadline exceeded
</code></pre>
<p>我们有一部分业务为对外开放的容器应用平台，其对客户实施按量计费。该 BUG 会导致额外计费。</p>
<p>Container Runtime 随 kubelet 一起升级。节点操作步骤变为</p>
<ol>
<li>kubectl drain <node> &ndash;ignore-daemonsets &ndash;delete-local-data</li>
<li>update kubelet</li>
<li>update runtime</li>
<li>kubectl uncordon <node></li>
</ol>
<p><code>update runtime</code> 对应 bash function 如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">function</span> switch-runtime<span class="o">()</span> <span class="o">{</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;start switch runtime: docker =&gt; containerd&#34;</span>
</span></span><span class="line"><span class="cl">    systemctl stop kubelet
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;kubelet stopped&#34;</span>
</span></span><span class="line"><span class="cl">    docker stop <span class="k">$(</span>docker ps -a -q<span class="k">)</span>
</span></span><span class="line"><span class="cl">    docker rm <span class="k">$(</span>docker ps -a -q<span class="k">)</span>
</span></span><span class="line"><span class="cl">    systemctl stop docker
</span></span><span class="line"><span class="cl">    format-container-disk <span class="c1"># formart disk for docker from lvm to ext4</span>
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;docker lvm disk formatted success&#34;</span>
</span></span><span class="line"><span class="cl">    systemctl stop docker
</span></span><span class="line"><span class="cl">    prepare-containerd <span class="c1"># download containerd binary and prepare config.toml</span>
</span></span><span class="line"><span class="cl">    start-containerd
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;containerd started&#34;</span>
</span></span><span class="line"><span class="cl">    sed -i <span class="s1">&#39;/KUBELET_ARGS=/a \--container-runtime=remote \\\n--container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\&#39;</span> /etc/kubernetes/kubelet.env
</span></span><span class="line"><span class="cl">    sed -i <span class="s1">&#39;s/--cgroup-driver=cgroupfs/--cgroup-driver=systemd/&#39;</span> /etc/kubernetes/kubelet.env
</span></span><span class="line"><span class="cl">    systemctl daemon-reload
</span></span><span class="line"><span class="cl">    systemctl restart kubelet
</span></span><span class="line"><span class="cl">    <span class="nb">echo</span> <span class="s2">&#34;end switch runtime: docker =&gt; containerd&#34;</span>
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>dockerd 所用 storage-driver 为 devicemapper，其采用 <a href="https://en.wikipedia.org/wiki/Logical_Volume_Manager_(Linux)">LVM</a> 为容器提供持久卷，较新版本已经废弃。步骤 <code>format-container-disk</code> 负责格式化 LVM 盘为 ext4，并将其挂载至路径 /var/lib/containerd。而在之前，脚本已经移除了所有 dockerd 容器并停止了 dockerd 服务。</p>
<p>详细内容见 <a href="/file/kube-upgrade.sh">kube-upgrade.sh</a> runtime 子命令。</p>
<h2 id="使用脚本简化操作">使用脚本简化操作</h2>
<p>前文 <a href="../2023-kubernetes-upgrade-1.14-1.21">v1.14.5 - v1.21.14 Kubernetes 跨版本升级记录</a> 列述了思路和每个步骤的操作方式。
最终操作中，所有步骤均模块化为如下子命令</p>
<ul>
<li>prepare: 在所有 master 节点上安装自 v1.15.12 至 v1.21.14 共 7 个版本的 kubeadm，kubelet 和 kubectl</li>
<li>cluster: 围绕 <code>kubeadm upgrade apply</code>，在首个 master 节点执行集群层面升级，升级 CoreDNS、kube-proxy、集群配置(kubeadm-config,kubelet-config) 以及该 master 节点上的 control plane，每个 minor 版本都需要操作</li>
<li>master &lt;mnode&gt;: 围绕 <code>kubeadm upgrade &lt;node&gt;</code>，升级对应 master 节点 kubelet 和 control plane，每个 minor 版本都需要操作</li>
<li>master-runtime &lt;mnode&gt;: 升级对应 master 节点 Container Runtime，可以在 control plane 升级至 1.21 后操作</li>
<li>worker &lt;wnode&gt;: 围绕 <code>kubeadm upgrade &lt;node&gt;</code>，升级对应 worker 节点 kubelet 和 Container Runtime，执行 1 次即可，直接自 1.14 升级至 1.21</li>
</ul>
<p>注：假设所有子命令均在某 master 节点执行，并假设该节点可以 ssh 登陆集群任意节点。</p>
<p>详细内容可查阅 <a href="/file/kube-upgrade.sh">kube-upgrade.sh</a>。</p>
<h2 id="经验---新-runtime-无法自-registry-拉取镜像">经验 - 新 Runtime 无法自 Registry 拉取镜像</h2>
<p>升级 Container Runtime 至 containerd v1.6.28 后，原本 dockerd 可以拉取的镜像，containerd 报 404。</p>
<pre><code>$ nerdctl pull my-register2.example.com/nginx:20231130134315-88-master
my-register2.example.com/nginx:20231130134315-88-master: resolving |-------------------------------|
elapsed:0.5 s                                            total: 0.0 B (0.0 B/s)
FATA[0000] httpReadseeker: failed open: content at https://my-register2.example.com/v2/nginx/manifests/
sha256:e086f32dcf9f00bae7a5d1c4fe7c9760b3af37fe222c28803fd9060d32e4fb27 not found: not found
</code></pre>
<p>背景是线上 registry 使用了较老版本，新版本 containerd 与其协议不兼容。解决方式便是升级 registry 为较新版本。</p>
<h2 id="教训---所有-ingress-节点同时升级导致全集群业务不可用">教训 - 所有 Ingress 节点同时升级导致全集群业务不可用</h2>
<p>节点（尤其是多个节点同时）升级时，使用 <code>kubectl drain</code> 排空，配合 PDB (<a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/">PodDisruptionBudget</a>) 可以保证应用可用实例始终保维持在一定数量或比例。</p>
<p>如下所示，设置 <code>maxUnavailable: 1</code> 则集群内应用 apigate 在同一时间段内只会有 1 个实例处于不可用状态。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-yaml" data-lang="yaml"><span class="line"><span class="cl"><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l">policy/v1</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l">PodDisruptionBudget</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">metadata</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l">apigate</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w"></span><span class="nt">spec</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">maxUnavailable</span><span class="p">:</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="c"># or &#39;10%&#39;</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">  </span><span class="nt">selector</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span><span class="w">
</span></span></span><span class="line"><span class="cl"><span class="w">      </span><span class="nt">app</span><span class="p">:</span><span class="w"> </span><span class="l">apigate</span><span class="w">
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>假设节点 A 和 B 之上均运行了 apigate 实例，这时尝试同时升级 A 和 B。先执行 <code>kubectl drain A</code>，这时 apigate 不可用为 1。紧接着执行 <code>kubectl drain B</code> 就会检测到 evict 操作会违背 apigate PDB。因此对应操作就进入等待重试状态，直到 apigate 不可用变回 0 时，才会继续执行。</p>
<p>因此，为关键服务设置 PDB 可避免其在集群升级期间不可用。</p>
<p>尽管在设计之初就为所有业务网关均设置了 PDB。但在操作小集群时，仍出现了 2 个 Ingress 节点同时升级导致所有业务流量短时间 500 的事故。</p>
<p>教训：不仅要为业务网关设置 PDB，还需为关键三方应用设置 PDB。典型如</p>
<ul>
<li>Prometheus，防止集群 Metrics 或告警出现丢失</li>
<li>Ingress，防止所有业务流量出现 500</li>
</ul>
<p>最后，可能还需为极端情况做预案。如某个 Ingress 节点长时间无法升级成功时，可采用添加新节点为 Ingress 节点的方式恢复。</p>
<h2 id="教训---boundserviceaccounttokenvolume-变动导致-ingress-nginx-新建容器启动失败">教训 - BoundServiceAccountTokenVolume 变动导致 Ingress Nginx 新建容器启动失败</h2>
<p>1.21 之后，ServiceAccount 绑定 Pod 默认方式由 Secret 挂载改为 ServiceAccountToken 挂载。</p>
<p><a href="../2023-kubernetes-upgrade-1.14-1.21">v1.14.5 - v1.21.14 Kubernetes 跨版本升级记录</a> 1.20 -&gt; 1.21 升级注意点提到，这个变动会导致新创建 Ingress Nginx Pod（ServiceAccountToken 挂载）与旧留存 Ingress Nginx Pod（ServiceAccountToken 挂载）存在不一致的情况。</p>
<p>对于 Container Image <code>quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.31.0</code> 而言，如果容器以非 root user 运行，读取 ServiceAccountToken 挂载文件会报错 <code>open /var/run/secrets/kubernetes.io/serviceaccount/token: permission denied</code>。</p>
<p>解决方式则是利用 <code>spec.securityContext.fsGroup</code> 更改挂载文件归属和权限。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">spec:
</span></span><span class="line"><span class="cl">  containers:
</span></span><span class="line"><span class="cl">    name: nginx-ingress-controller
</span></span><span class="line"><span class="cl">    securityContext:
</span></span><span class="line"><span class="cl">      allowPrivilegeEscalation: true
</span></span><span class="line"><span class="cl">      capabilities:
</span></span><span class="line"><span class="cl">        add:
</span></span><span class="line"><span class="cl">        - NET_BIND_SERVICE
</span></span><span class="line"><span class="cl">        drop:
</span></span><span class="line"><span class="cl">        - ALL
</span></span><span class="line"><span class="cl">      runAsUser: 101
</span></span><span class="line"><span class="cl">+ securityContext: # fix 
</span></span><span class="line"><span class="cl">+   fsGroup: 65534 # fix
</span></span></code></pre></td></tr></table>
</div>
</div><p>教训：重视 Pod API 变更，升级集群时，应尽快将所有 Pod 滚动至最新版本。</p>
<h2 id="裸金属环境-apiserver-真是高可用吗">裸金属环境 apiserver 真是高可用吗</h2>
<p>升级某个集群时，预期架构为所有 worker 节点通过 keepalived virtual IP 连接至某台 apiserver。这样可以假设 apiserver-1 处于升级期间，流量可以转发至 apiserver-2 或 apiserver-3。</p>
<p>实际情况是，该集群 keepalived 背后 apiserver-2 节点和 apiserver-3 节点在某次操作被注释掉了，并且该配置之前没有入库。
故操作该集群升级时，发生了所有 worker 节点与 apiserver 断连的事故。</p>
<pre><code> worker-1     worker-2      worker-3                          worker-1     worker-2      worker-3
    |             |            |                                |             |            | 
    +-------------+------------+                                +-------------+------------+
                  |                                                           |
      keepalived virtual IP             ---- actual ---&gt;          keepalived virtual IP 
                  |                                                           |
    +-------------+------------+                                +-------------+------------+
    |             |            |                                |             x            x
apiserver-1  apiserver-2  apiserver-3                       apiserver-1  apiserver-2  apiserver-3
</code></pre>
<p>教训：敬畏生产，明晰 apiserver 架构，操作前做好检查</p>
<p>注：在较新裸金属集群中，已经弃用 keepalived virtual IP 方案，改用了客户端负载均衡方案。即 haproxy 作为 static Pod 跑在 worker 节点，为 kubelet 至 kube-apiserver 提供负载均衡</p>
<pre><code>               worker-1 
                  |
                haproxy     
                  |          
    +-------------+-----------+
    |             |           |
apiserver-1  apiserver-2  apiserver-3 
    |             |           |
    +-------------+-----------+ 
                  |
                haproxy     
                  |
               worker-2
</code></pre>
<h2 id="经验---runtime-切换之-gpu-节点处理">经验 - Runtime 切换之 GPU 节点处理</h2>
<p>旧版 Docker 支持 NVIDIA GPU 之 runtime 相关配置 (/etc/docker/daemon.json) 如下</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-json" data-lang="json"><span class="line"><span class="cl"><span class="p">{</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;default-runtime&#34;</span><span class="p">:</span> <span class="s2">&#34;nvidia&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">  <span class="nt">&#34;runtimes&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="nt">&#34;nvidia&#34;</span><span class="p">:</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;path&#34;</span><span class="p">:</span> <span class="s2">&#34;/usr/bin/nvidia-container-runtime&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">      <span class="nt">&#34;runtimeArgs&#34;</span><span class="p">:</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">  <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>切换到 containerd 应对应为</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-toml" data-lang="toml"><span class="line"><span class="cl">    <span class="p">[</span><span class="nx">plugins</span><span class="p">.</span><span class="s2">&#34;io.containerd.grpc.v1.cri&#34;</span><span class="p">.</span><span class="nx">containerd</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">      <span class="nx">default_runtime_name</span> <span class="p">=</span> <span class="s2">&#34;nvidia&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="nx">snapshotter</span> <span class="p">=</span> <span class="s2">&#34;overlayfs&#34;</span>
</span></span><span class="line"><span class="cl">      <span class="p">[</span><span class="nx">plugins</span><span class="p">.</span><span class="s2">&#34;io.containerd.grpc.v1.cri&#34;</span><span class="p">.</span><span class="nx">containerd</span><span class="p">.</span><span class="nx">runtimes</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="p">[</span><span class="nx">plugins</span><span class="p">.</span><span class="s2">&#34;io.containerd.grpc.v1.cri&#34;</span><span class="p">.</span><span class="nx">containerd</span><span class="p">.</span><span class="nx">runtimes</span><span class="p">.</span><span class="nx">nvidia</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">          <span class="nx">runtime_type</span> <span class="p">=</span> <span class="s2">&#34;io.containerd.runc.v2&#34;</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="nx">plugins</span><span class="p">.</span><span class="s2">&#34;io.containerd.grpc.v1.cri&#34;</span><span class="p">.</span><span class="nx">containerd</span><span class="p">.</span><span class="nx">runtimes</span><span class="p">.</span><span class="nx">nvidia</span><span class="p">.</span><span class="nx">options</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="nx">BinaryName</span> <span class="p">=</span> <span class="s2">&#34;/usr/bin/nvidia-container-runtime&#34;</span>
</span></span><span class="line"><span class="cl">            <span class="nx">systemdCgroup</span> <span class="p">=</span> <span class="kc">true</span>
</span></span><span class="line"><span class="cl">        <span class="c"># 填写多个 runtimes 以便用户使用 K8s RuntimeClass 自由切换 https://kubernetes.io/docs/concepts/containers/runtime-class/</span>
</span></span><span class="line"><span class="cl">        <span class="c">#[plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.runc]</span>
</span></span><span class="line"><span class="cl">        <span class="c">#  runtime_type = &#34;io.containerd.runc.v2&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="c">#  [plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.runc.options]</span>
</span></span><span class="line"><span class="cl">        <span class="c">#    systemdCgroup = true</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于部分 Ubuntu 版本 (Ubuntu 16.04.2, Linux 4.4.0)，笔者在升级后遇到了部分 Pod 无法启动、kubelet 报错无法处理 Cgroups 的情况</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">Nov 04 15:20:34 mynode kubelet[60862]: W1104 15:20:34.892535 60862 watcher.go:95] Error while processing event (&#34;/sys/fs/cgroup/devices/libcontainer_50995_systemd_test_default.slice&#34;: 0x40000100 == IN_CREATE| IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/libcontainer_50995_systemd_test_default.slice: no such file or directory
</span></span></code></pre></td></tr></table>
</div>
</div><p>原因为运行时链路自 <code>dockerd -&gt; nvidia-container-runtime</code> 改为 <code>containerd -&gt; nvidia-container-runtime</code> 后，
nvidia-container-runtime 仍会默认调用 docker-runc，而不是更新后的较新 runc。而 docker-runc (version 1.0.0-rc5+dev.docker-18.06) 在 Ubuntu 16.04.2 上处理 systemd driver cgroup 有点问题</p>
<p>解决方式是修改配置文件 <code>/etc/nvidia-container-runtime/config.toml</code>，让 nvidia-container-runtime 优先使用 runc 而非 docker-runc</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># Specify the runtimes to consider. This list is processed in order and the PATH
</span></span><span class="line"><span class="cl"># searched for matching executables unless the entry is an absolute path.
</span></span><span class="line"><span class="cl">runtimes = [
</span></span><span class="line"><span class="cl">    &#34;runc&#34;,
</span></span><span class="line"><span class="cl">    &#34;docker-runc&#34;,
</span></span><span class="line"><span class="cl">]
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="结语设计可持续演进的软件体系">结语：设计可持续演进的软件体系</h2>
<p>本次升级只涉及 Kubernetes 容器组件，仍然不包括</p>
<ul>
<li>Ingress Controller 更新</li>
<li>Calico CNI 更新</li>
<li>Prometheus 更新</li>
<li>Linux kernel 更新</li>
</ul>
<p>旧版本可能会有 K8s 兼容问题。以 Nginx Ingress Controller 为例，其就有<a href="https://github.com/kubernetes/ingress-nginx?tab=readme-ov-file#supported-versions-table">明确的版本兼容列表</a>。而在升 1.21 过程中，随着控制面组件渐次废弃 HTTP 明文端口，老旧 Prometheus 爬取规则也出现了不兼容的情况。</p>
<p>应同步更新应用级组件，通常做到这点并不难。</p>
<p>中小公司不会有专门的基础架构团队分别负责网络、存储、监控、Kubernetes 等。单靠业务团队小部分人兼职处理所有更新，是不可能完成的任务。走流程提需求至运维团队，对方可能会告诉你，没有现成工具可以升级 kernel、Kubernetes。追至负责负责工具的团队时，往往又会以优先级不够之类的理由，要么排期靠后，要么不予处理。</p>
<p>而随着时间流逝，人事更迭，你会发现，24 年做业务，仍旧用着 21 年的 Go 1.16，19 年的 Kubernetes 和 16 年的 Ubuntu 16.04 乃至 10 多年前的 CentOS 6。</p>
<p>一名优秀的工程管理者在引入三方依赖之前，首先应该想清楚已有依赖能否解决业务问题。如果是，则慎重引入。比如当前团队 100% Java，Java 能解决好业务需求，那么就不要为了某些酷炫特性引入 Golang。</p>
<p>如若决定引入，那么就应设计好依赖的持续演进方案。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Zeng Xu</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2024-04-10 17:07
        
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content">本作品采用 <a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议</a> 进行许可，转载时请注明原文链接。</span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/kubernetes/">kubernetes</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/2025-knative-pod-autoscaler/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">Dive into Knative Pod Autoscaler</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/2023-retriable-http-client/">
            <span class="next-text nav-default">Notes on Retriable HTTP Client (with Golang/Rust example</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        <div id="gitalk-container"></div>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js" crossorigin="anonymous"></script>
    <script type="text/javascript">
      var gitalk = new Gitalk({
        id: '2024-03-26 15:31:54 \u002b0800 CST',
        title: 'Kubernetes v1.14.5 → v1.21.14 升级补遗及经验教训',
        clientID: '6ab3c721bb197ea92f1e',
        clientSecret: '217d38cc1905f60f1d963c555be606ed3e707937',
        repo: 'phosae.github.io',
        owner: 'phosae',
        admin: ['phosae'],
        body: decodeURI(location.href)
      });
      gitalk.render('gitalk-container');
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/gitalk/gitalk">comments powered by gitalk.</a></noscript>

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:zenngxu@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/phosae" class="iconfont icon-github" title="github"></a>
      <a href="https://weibo.com/u/1566013967" class="iconfont icon-weibo" title="weibo"></a>
  <a href="https://www.zeng.dev/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2019 - 
    2025<span class="heart"><i class="iconfont icon-heart"></i></span><span>Zeng Xu</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>



<script type="text/javascript" src="/js/main.min.4ae89da218555efa0e7093a20b92017d2e1202b66fff9fc2edf4cb8d44b44c6e.js"></script>


  
    
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-FEPN2KZF84"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-FEPN2KZF84');
        }
      </script>
    
  











</body>
</html>
